{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have saved the vector stores of each superheros, we can now go ahead with building the RAG-based chatbot. In this section, we will be building the chatbot using the Haystack library. The Haystack library is a framework for building end-to-end search pipelines that enable us to build powerful search and QA systems. Haystack at the implementation level is very intuitive and easy to use. Like any other RAG-based application, this blog will also have the following components:\n",
    "- Embeddings for the query - We will be using the FastEmbed model for creating the embeddings of the query.\n",
    "- Retriever - This retrieves the relevant documents from the document store based on the query embeddings.\n",
    "- prompt_builder - This is used to build the prompt template for the RAG model.\n",
    "- Generator - This generates the answer to the query based on the prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join as pjoin\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '..'\n",
    "data_folder = 'data'\n",
    "script_folder = 'scripts'\n",
    "dialogue_folder = 'dialogues'\n",
    "config_file = 'config.yaml'\n",
    "embed_dim = 384\n",
    "vector_store_name = 'QDRANT_VECTOR_DATABASE'\n",
    "vector_store_path = pjoin(root, vector_store_name)\n",
    "embedding_model = 'BAAI/bge-small-en-v1.5'\n",
    "superhero = 'Deadpool'\n",
    "config_path = pjoin(root, config_file)\n",
    "llm_model = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "max_new_tokens = 250\n",
    "number_of_documents_to_retrieve = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deepkapha/anaconda3/envs/ai-chatbot/lib/python3.12/site-packages/torch/cuda/__init__.py:628: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack_integrations.document_stores.qdrant import QdrantDocumentStore\n",
    "from haystack_integrations.components.retrievers.qdrant import QdrantEmbeddingRetriever\n",
    "from haystack.components.generators import HuggingFaceTGIGenerator\n",
    "from haystack_integrations.components.embedders.fastembed import FastembedTextEmbedder\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack.components.others import Multiplexer\n",
    "\n",
    "def get_superhero_names(superhero, config_path):\n",
    "    '''\n",
    "    This function returns a list of superhero names and their synonyms\n",
    "    '''\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    superhero_synonym = config['SUPERHERO_SYNONYMS'][superhero][0]\n",
    "    superhero_names = [superhero.upper(), superhero_synonym.upper(), superhero_synonym.replace(' ', '-').upper()]\n",
    "    superhero_names = list(set(superhero_names))\n",
    "    return superhero_names\n",
    "\n",
    "def load_document_store(vector_store_path, superhero, embed_dim):\n",
    "    '''\n",
    "    This function loads the QdrantDocumentStore that will be used by retriever to fetch the context documents\n",
    "    '''\n",
    "    document_store = QdrantDocumentStore(\n",
    "        path=vector_store_path,\n",
    "        index=superhero,\n",
    "        embedding_dim=embed_dim,\n",
    "    )\n",
    "    return document_store\n",
    "\n",
    "def build_prompt():\n",
    "    '''\n",
    "    This function builds the prompt template that will be used by the PromptBuilder\n",
    "    '''\n",
    "    prompt = \"\"\"\n",
    "    You are a helpful AI assistant that mimics the tone of the specified character based on provided context documents. \n",
    "    Use the context to capture and replicate the character's tone accurately.\n",
    "\n",
    "    You will be given a set of CONTEXT documents, which you should use to understand and replicate the character's tone in your response. \n",
    "    The context should primarily inform the tone rather than the content of your answer. \n",
    "    You may answer questions without the context if it is not necessary, but always ensure your tone matches that of the character.\n",
    "\n",
    "    Respond without prefacing with phrases like \"Based on the context...\" or \"I think...\".\n",
    "\n",
    "    If the context is not necessary to answer the question, you may ignore it. \n",
    "    You may also use your own knowledge of the character tone to answer the question in the same tone.\n",
    "\n",
    "    When you start your response, ensure that it is clear that you are answering the question.\n",
    "    Do not say things like \"Please respond with the tone of...\". Just directly answer the question in the character's tone.\n",
    "\n",
    "    ******************************************\n",
    "    -----------CONTEXT STARTS HERE------------\n",
    "    {% for doc in documents %}\n",
    "        {{ doc.content }}\n",
    "    ------------------------------------------\n",
    "    {% endfor %}\n",
    "    -----------CONTEXT ENDS HERE------------\n",
    "    ******************************************\n",
    "    Copy the tone of these charachters : {{ superhero_names }} dialogue and answer the following question:\n",
    "    ******************************************\n",
    "    Question: {{ query }}\n",
    "    ******************************************\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the prompt builder\n",
    "    prompt_builder = PromptBuilder(template=prompt)\n",
    "    return prompt_builder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_documents_to_retrieve = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "superhero_names = get_superhero_names(superhero, config_path)\n",
    "document_store = load_document_store(vector_store_path, superhero, embed_dim)\n",
    "prompt_builder = build_prompt()\n",
    "\n",
    "generator = HuggingFaceTGIGenerator(model=llm_model, generation_kwargs={\"max_new_tokens\": max_new_tokens})\n",
    "retriever = QdrantEmbeddingRetriever(document_store=document_store, top_k=number_of_documents_to_retrieve)\n",
    "embedder = FastembedTextEmbedder(model = embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"Who are the Avengers?\"\n",
    "# embedder.warm_up()\n",
    "# embeddings = embedder.run(query)\n",
    "# docs = retriever.run(query_embedding=embeddings['embedding'])\n",
    "\n",
    "# prompt = prompt_builder.run(superhero_names=superhero_names, documents=docs['documents'][:1], query=query)\n",
    "# print(prompt['prompt'])\n",
    "\n",
    "# generator.warm_up()\n",
    "# print(generator.run(prompt=prompt['prompt'])['replies'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rag_pipeline(embedder, retriever, prompt_builder, generator):\n",
    "    '''\n",
    "    This is the main function that builds the RAG pipeline\n",
    "    We start by creating a Pipeline object and adding the components to it\n",
    "    We then connect the components to each other. This connection essentially defines the flow of data between the components\n",
    "    '''\n",
    "    rag = Pipeline()\n",
    "\n",
    "    rag.add_component(instance=Multiplexer(str), name=\"multiplexer\")\n",
    "\n",
    "    rag.add_component(\"embedder\", embedder)\n",
    "    rag.add_component(\"retriever\", retriever)\n",
    "    rag.add_component(\"prompt\", prompt_builder)\n",
    "    rag.add_component(\"llm\", generator)\n",
    "\n",
    "    rag.connect(\"multiplexer.value\", \"embedder.text\")\n",
    "    rag.connect(\"multiplexer.value\", \"prompt.query\")\n",
    "\n",
    "    rag.connect(\"embedder.embedding\", \"retriever.query_embedding\")\n",
    "    rag.connect(\"retriever.documents\", \"prompt.documents\")\n",
    "    rag.connect(\"prompt.prompt\", \"llm\")\n",
    "\n",
    "    return rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = build_rag_pipeline(embedder, retriever, prompt_builder, generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have built the RAG pipeline using the Haystack library. We first start by getting the superhero names and then loading the document store. We then build the prompt template and finally build the RAG pipeline using the FastEmbed model for creating the embeddings of the query. The QdrantEmbeddingRetriever is used to retrieve the relevant documents from the document store based on the query embeddings. The HuggingFaceTGIGenerator is used to generate the answer to the query based on the prompt template. Finally, we build the RAG pipeline using the embedder, retriever, prompt_builder, and generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One interesting thing about the Haystack library is that it allows us to visualize the RAG pipeline. We can visualize the pipeline using the `pipeline.show()` method. This will show the pipeline in a graphical format. This visualization is very helpful in understanding the flow of the pipeline and how the different components are connected to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rag.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22551080d5254354b0a1c4f48b9b5734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deepkapha/anaconda3/envs/ai-chatbot/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Calculating embeddings: 100%|██████████| 1/1 [00:00<00:00, 18.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Please respond with the tone of Wade Wilson (Deadpool). \n",
      "\n",
      "(Note: I'll be using the provided context to capture the tone of Wade Wilson. If you need any further clarification, feel free to ask!)\n"
     ]
    }
   ],
   "source": [
    "question = \"Tell me about you\"\n",
    "\n",
    "pipeline_input = {\n",
    "    \"multiplexer\": {\n",
    "        \"value\": question,\n",
    "    },\n",
    "    \"prompt\": {\n",
    "        \"superhero_names\": superhero_names\n",
    "    }\n",
    "}\n",
    "\n",
    "result = rag.run(pipeline_input)\n",
    "response = result['llm']['replies'][0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now that we have built the RAG pipeline, we can now go ahead and test the chatbot. We can test the chatbot by asking questions about the superheros. The chatbot will retrieve the relevant information from the document store and generate the answer to the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in this section, we will be building a user interface for the character chatbot. We will be using the Streamlit library to build the user interface. Streamlit is a very powerful library that allows us to build interactive web applications using simple Python scripts. We will be building a simple web application that will allow the user to select a superhero from the dropdown list and ask questions to the virtual superhero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rag.show()\n",
    "# rag.draw(\"pipeline.png\")\n",
    "# print(rag.dumps())\n",
    "# pipe = Pipeline.loads(pipeline_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
